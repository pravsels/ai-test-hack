{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmYy6vLcRUsJ",
        "outputId": "beff239d-1da3-4659-f9c0-4bc892df4653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trl==0.0.3\n",
            "  Downloading trl-0.0.3-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.8/dist-packages (from trl==0.0.3) (1.21.6)\n",
            "Collecting transformers==4.3.2\n",
            "  Downloading transformers-4.3.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from trl==0.0.3) (1.13.0+cu116)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2->trl==0.0.3) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2->trl==0.0.3) (2022.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2->trl==0.0.3) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2->trl==0.0.3) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 84.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.3.2->trl==0.0.3) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->trl==0.0.3) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.3.2->trl==0.0.3) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2->trl==0.0.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2->trl==0.0.3) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2->trl==0.0.3) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.3.2->trl==0.0.3) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2->trl==0.0.3) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2->trl==0.0.3) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.3.2->trl==0.0.3) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=78f5f26d853df52133d8759c2e38940b8840fe6f02a73928360dc8c5aa4700c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers, trl\n",
            "Successfully installed sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.3.2 trl-0.0.3\n"
          ]
        }
      ],
      "source": [
        "# install the TRL library \n",
        "!pip install trl==0.0.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8MhPEwITHFa",
        "outputId": "bf7ab583-27f6-4e99-9dc1-ef5f190da1c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.3.2)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.7-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 92.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 81.7 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 89.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 86.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 78.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers) (0.0.53)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 92.8 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 91.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 92.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 84.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 72.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 95.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 89.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 92.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 98.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 99.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 94.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 91.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 93.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 88.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 90.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 99.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers) (1.2.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=47994506415e4a6399d97105f4a3bb52b1fcf35bff98c831e17f47dc01ba7f6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: urllib3, smmap, gitdb, xxhash, shortuuid, setproctitle, sentry-sdk, responses, pathtools, multiprocess, huggingface-hub, GitPython, docker-pycreds, wandb, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.29 datasets-2.7.1 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.11.1 multiprocess-0.70.14 pathtools-0.1.2 responses-0.18.0 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 urllib3-1.25.11 wandb-0.13.7 xxhash-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# auto-reload external modules that might've changed \n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "nmCfe2iSRfwA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import wandb\n",
        "import time\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import choices\n",
        "import matplotlib.pyplot as plt\n",
        "tqdm.pandas()\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
        "from trl.ppo import PPOTrainer\n",
        "from trl.core import build_bert_batch_from_txt"
      ],
      "metadata": {
        "id": "mPRAer8LSWdI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"lm_name\": \"lvwerra/gpt2-imdb\",       # LLM\n",
        "    \"ref_lm_name\": \"lvwerra/gpt2-imdb\",   # reference LLM (same as above)\n",
        "    \"cls_model_name\": \"lvwerra/distilbert-imdb\",  # BERT classification model \n",
        "    \"tk_name\": \"gpt2\",  # tokenizer name\n",
        "    \"steps\": 51200,\n",
        "    \"batch_size\": 256,\n",
        "    \"forward_batch_size\": 16,\n",
        "    \"ppo_epochs\": 4,   \n",
        "    \"txt_in_len\": 5,\n",
        "    \"txt_out_len\": 20,\n",
        "    \"lr\": 1.41e-5,\n",
        "    \"init_kl_coef\": 0.2,\n",
        "    \"target\": 6,\n",
        "    \"horizon\": 10000,\n",
        "    \"gamma\": 1,\n",
        "    \"lam\": 0.95,\n",
        "    \"cliprange\": .2,\n",
        "    \"cliprange_value\": .2,\n",
        "    \"vf_coef\": .1, \n",
        "    \"seed\": 1,\n",
        "}"
      ],
      "metadata": {
        "id": "6RB2Bf0oSox2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(config['seed'])"
      ],
      "metadata": {
        "id": "idpPfbhWTvCk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `gpt2_imdb` model was fine-tuned on the IMDB dataset for 1 epoch with the huggingface script (no special settings). The other parameters are mostly taken from the original paper [\"Fine-Tuning Language Models from Human Preferences\"](https://arxiv.org/pdf/1909.08593.pdf). This model as well as the BERT model is available in the Huggingface model zoo [here](https://huggingface.co/models)."
      ],
      "metadata": {
        "id": "QXXUYpCmUNdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log all metrics during training \n",
        "wandb.init(name='long-response', project='gpt2-ctrl', config=config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "DX3JsE8PUSfI",
        "outputId": "0bb5e0ea-a4d1-41cb-bfb8-7d38ec975f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load imdb with datasets\n",
        "dataset = load_dataset('imdb', split='train')\n",
        "dataset = dataset.rename_columns({'text': 'review', 'label': 'sentiment'})\n",
        "dataset.set_format('pandas')\n",
        "df = dataset[:]\n",
        "\n",
        "# make sure the reviews are long enough (>500)\n",
        "df = df.loc[df['review'].str.len() > 500]\n",
        "\n",
        "# but still less than 1000\n",
        "df['review'] = df['review'].apply(lambda x: x[:1000])\n",
        "\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "0pNckObaU9tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load in the BERT sentiment classifier & tokenizer\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(config['cls_model_name'])\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(config['cls_model_name'])"
      ],
      "metadata": {
        "id": "VelJVN11W3Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model outputs are the logits for the negative and positive class. We will use the logits for the positive class as the reward signal for fine-tuning the GPT2 LLM."
      ],
      "metadata": {
        "id": "AAfJcRpWZK8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'this movie gave me crippling depression and pushed me ever so slightly to the precipice.'\n",
        "output = sentiment_model.forward(sentiment_tokenizer.encode(text, return_tensors=\"pt\")) # pt returns pytorch tensors\n",
        "output"
      ],
      "metadata": {
        "id": "vuPh3Z-vZbnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'movies like this should not be made. what a colossal waste of time!'\n",
        "output = sentiment_model.forward(sentiment_tokenizer.encode(text, return_tensors=\"pt\"))\n",
        "output"
      ],
      "metadata": {
        "id": "86AfilPJZxg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"watching morbius has been the single greatest event in my life so far. I was in a state of pure bliss from start to finish.\"\n",
        "output = sentiment_model.forward(sentiment_tokenizer.encode(text, return_tensors=\"pt\"))\n",
        "output"
      ],
      "metadata": {
        "id": "yGL3DWrLagfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output[0]"
      ],
      "metadata": {
        "id": "cwB9ybOxax_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the positive logit will be the reward signal\n",
        "output[0][0, 1]"
      ],
      "metadata": {
        "id": "9ZexoBr8a-K2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pre-trained GPT2 models, one for fine-tuning and another for reference. This is so that we can calculate KL-divergence between the models as we fine-tune. This score will be incorporated into the reward signal in the PPO training so that the LLM being fine-tuned doesn't deviate too much from the referene LLM. "
      ],
      "metadata": {
        "id": "EqTm_8RFbtr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_model = GPT2HeadWithValueModel.from_pretrained(config['lm_name'])\n",
        "gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained(config['ref_lm_name'])\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(config['tk_name'])"
      ],
      "metadata": {
        "id": "gJvoTQeecSY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "alC9drEodBUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move models to device \n",
        "_ = gpt2_model.to(device)\n",
        "_ = gpt2_model_ref.to(device)\n",
        "_ = sentiment_model.to(device)"
      ],
      "metadata": {
        "id": "DC0G3PIfdPqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log the gradients and weights of the model during training\n",
        "wandb.watch(gpt2_model, log='all')"
      ],
      "metadata": {
        "id": "KWQ1jj4JdkdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize reviews and clilp to max text len\n",
        "df['tokens'] = df['review'].progress_apply(lambda x: gpt2_tokenizer.encode(' '+x, return_tensors=\"pt\").to(device)[0, :config['txt_in_len']])"
      ],
      "metadata": {
        "id": "vZi22bBadr7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# as well as detokenize into queries for display\n",
        "df['query'] = df['tokens'].progress_apply(lambda x: gpt2_tokenizer.decode(x))"
      ],
      "metadata": {
        "id": "yhba0nWz0YDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['query'].head()"
      ],
      "metadata": {
        "id": "16zu9OLH3BHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each query needs to be appended with the control token to signal to the model what target sentiment we aim to generate. "
      ],
      "metadata": {
        "id": "ENTb-XI75h-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ctrl_str = ['[negative]', '[neutral]', '[positive]']\n",
        "ctrl_tokens = dict((s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
      ],
      "metadata": {
        "id": "aI2i0p9r4__x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctrl_tokens"
      ],
      "metadata": {
        "id": "CzSmEKtb6B1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a reward function that takes in logits and scales it according to"
      ],
      "metadata": {
        "id": "xFI0KcOZ7XPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def positive_logit_to_reward(logit, task):\n",
        "  \"\"\"\n",
        "  Take the positive sentiment logit and scale it for the task.\n",
        "    task [negative]: reward = -logit\n",
        "    task [neutral]: reward = -2*abs(logit)+4\n",
        "    task [positive]: reward = logit\n",
        "  \"\"\"\n",
        "  for i in range(len(logit)):\n",
        "    if task[i] == '[negative]':\n",
        "      logit[i] = -logit[i]\n",
        "    elif task[i] == '[neutral]':\n",
        "      logit[i] = -2 * abs(logit) + 4\n",
        "    elif task[i] == '[positive]':\n",
        "      pass \n",
        "    \n",
        "    else:\n",
        "      raise ValueError('task has to be in [0, 1, 2]')\n",
        "    \n",
        "    return logit"
      ],
      "metadata": {
        "id": "UjlQuWyZ8I9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the positive logit is passed in 3 times, for each sentiment \n",
        "positive_logit_to_reward(torch.Tensor([4,4,4]), ctrl_str)"
      ],
      "metadata": {
        "id": "JSLm7eNJ88qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training loop consists of the following steps: \n",
        "*   Get a batch of queries and create random controls \n",
        "*   Get query responses from the LLM \n",
        "*   Join query and responses and tokenize for BERT input \n",
        "*   Get sentiments for query / response pairs from BERT\n",
        "*   Optimize policy with PPO (query, response, reward) triplet\n",
        "*   Log all training statistics \n"
      ],
      "metadata": {
        "id": "M61DGmivD8JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setup PPO trainer \n",
        "ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, **config)\n",
        "fbs = config['forward_batch_size']\n",
        "\n",
        "for epoch in tqdm(range(int(np.ceil(config['steps']/config['batch_size'])))):\n",
        "  # empty cache for every run \n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  logs = dict()\n",
        "  game_data = dict()\n",
        "  timing = dict()\n",
        "  t0 = time.time()\n",
        "\n",
        "  # get a batch from the dataset and annotate tasks \n",
        "  df_batch = df.sample(config['batch_size'])\n",
        "  task_list = choices(ctrl_str, k=config['batch_size']) # pick list of batch_size ctr_str indices at random\n",
        "  task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
        "  query_list = df_batch['query'].tolist()\n",
        "  game_data['query'] = [t + q for t, q in zip(task_list, query_list)]\n",
        "\n",
        "  query_tensors = torch.stack(df_batch['tokens'].tolist())  # tokenized queries \n",
        "  query_tensors = torch.cat((task_tensors, query_tensors), axis=1)  # concatenate tokenized control string with tokenized queries\n",
        "\n",
        "  # get response from GPT2\n",
        "  t = time.time()\n",
        "  response_tensors = []\n",
        "  # feed in queries with lower batch size (fbs), so as to avoid out of memory error \n",
        "  for i in range(int(config['batch_size']/fbs)):\n",
        "    response = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs], txt_len=config['txt_out_len'])\n",
        "    response_tensors.append(response)\n",
        "  response_tensors = torch.cat(response_tensors)  # list is concatenated to tensors \n",
        "  \n",
        "  game_data['response'] = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])]\n",
        "  timing['time/gpt2_response'] = time.time() - t\n",
        "\n",
        "  # tokenize text for sentiment analysis \n",
        "  t = time.time()\n",
        "  texts = [q + r for q,r in zip(query_list, game_data['response'])] # query + response for BERT input \n",
        "  sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)\n",
        "  timing['time/build_bert_input_sentiment'] = time.time() - t\n",
        "\n",
        "  # get sentiment score \n",
        "  t = time.time()\n",
        "  positive_logits = []\n",
        "  for i in range(int(config['batch_size']/fbs)):\n",
        "    res = sentiment_model.forward(sentiment_inputs[i*fbs:(i+1)*fbs], \n",
        "                                  attention_masks[i*fbs:(i+1)*fbs])[0][:, 1].detach()\n",
        "    positive_logits.append(res)\n",
        "\n",
        "  rewards = positive_logit_to_reward(torch.cat(positive_logits), task_list)\n",
        "  timing['time/get_sentiment_preds'] = time.time() - t\n",
        "\n",
        "  # run PPO training \n",
        "  t = time.time()\n",
        "  stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "  timing['time/ppo_optimization'] = time.time() - t\n",
        "\n",
        "  # log everything \n",
        "  timing['time/epoch'] = time.time()-t0\n",
        "  table_rows = [list(r) for r in zip(game_data['query'], game_data['response'], rewards.cpu().tolist())]\n",
        "  logs.update({'game_log':wandb.Table(\n",
        "      columns=['query', 'response', 'reward'],\n",
        "      rows=table_rows)})\n",
        "  \n",
        "  logs.update(timing)\n",
        "  logs.update(stats)\n",
        "\n",
        "  logs['env/reward_mean'] = torch.mean(rewards).cpu().numpy()\n",
        "  logs['env/reward_std'] = torch.std(rewards).cpu().numpy()\n",
        "  logs['env/reward_dist'] = rewards.cpu().numpy()\n",
        "\n",
        "  for ctrl_s in ctrl_str:\n",
        "      key = 'env/reward_'+ctrl_s.strip('[]')\n",
        "      logs[key] = np.mean([r for r, t in zip(logs['env/reward_dist'], task_list) if t==ctrl_s])\n",
        "\n",
        "  wandb.log(logs)"
      ],
      "metadata": {
        "id": "0QnYWl3EE4lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QgrpFIn2aziu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}